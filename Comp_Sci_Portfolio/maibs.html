<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Portfolio | Goodnews Babade</title>
  <link rel="stylesheet" href="style.css">
  <link rel="icon" type="image/x-icon" href="./79184c4a98ba866bd6ef244e69dddeadbe4e9d6edbc621ec1de4d4000b1abdbd.0-removebg-preview.png">
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>

  <header class="header">
    <nav class="navbar">
      <div class="logo"><a href="index.html">Goodnews Babade</a></div>
      <ul class="nav-links">
        <li><a href="about.html">About</a></li>
        <li><a href="portfolio.html">Portfolio</a></li>
        <li><a href="skills.html">Skills</a></li>
        <li><a href="contact.html">Contact</a></li>
        <li><a href="hobbies.html">Hobbies</a></li>
      </ul>
    </nav>
  </header>

  <section class="project">
    <div class="container">
      <h1>A Multimodal Artificial Intelligence Benchmarking System: Emotion Classification and Self-Evaluation in Music Using Audio, Lyrics, and LLM</h1>
      <h3>Python/Machine Learning/AI</h3>
      <hr>
      <br>
      <img src="./emotionproject.png" alt="Multimodal AI Benchmarking System" width="640">
      <p><b><u><i>Description: </i></u></b><br>
        This project explores how artificial intelligence can interpret and evaluate emotional expression in music by integrating multiple modalities:audio features, lyrical semantics, and large language model (LLM) reasoning. The goal is to measure not only how well AI can classify emotion, but also how confidently and consistently it understands its own predictions.
      </p><br>

      <p><b><u><i>Question: </i></u></b><br>
        How accurately can multimodal AI systems classify musical emotion when combining auditory, textual, and linguistic reasoning data, and how reliable are their self-assessed confidence scores across modalities?
      </p><br>

      <p><b><u><i>Solution: </i></u></b><br>
        A benchmarking framework was developed comparing:
        <ul>
          <li>A gradient boosting classifier trained on acoustic features (tempo, key, spectral contrast, MFCCs)</li>
          <li>A zero-shot large language model (OpenChat 3.5) interpreting song lyrics</li>
        </ul>
        The system evaluates both accuracy and calibration (expected calibration error), and analyzes disagreement between modalities using statistical tests such as Cohen’s κ and McNemar’s test.
      </p><br>

      <p><b><u><i>Key Features & Tools: </i></u></b><br>
        <ul>
          <li>Python, Scikit-learn, TensorFlow, and Librosa for audio feature extraction</li>
          <li>LLM-based emotion inference and confidence scoring</li>
          <li>Multimodal evaluation metrics: Accuracy, Macro-F1, ECE, and Agreement Statistics</li>
          <li>Music4All Dataset for large-scale validation</li>
        </ul>
      </p><br>
    </div>
  </section>

  <footer class="footer">
    <p>&copy; 2024 Goodnews Babade. All rights reserved.</p>
  </footer>
</body>
</html>